{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoPr/WRmr5o/QQR88sEqZY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marioaloam-00/practica3-cloudrun/blob/main/Patentspy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Patent Data Analysis on Dataproc Cluster cluster-aa01\n",
        "Run with: spark-submit --master yarn patent_analysis.py\n",
        "\"\"\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend for cluster\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import gc\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "class PatentDataAnalyzerSpark:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize Spark session for Dataproc cluster\n",
        "        \"\"\"\n",
        "        self.spark = None\n",
        "        self.df = None\n",
        "        self.total_rows = 0\n",
        "\n",
        "    def create_spark_session(self):\n",
        "        \"\"\"Create and configure Spark session for Dataproc\"\"\"\n",
        "        print(\"Initializing Spark session on cluster-aa01...\")\n",
        "\n",
        "        self.spark = SparkSession.builder \\\n",
        "            .appName(\"PatentDataAnalysis\") \\\n",
        "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "            .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
        "            .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
        "            .config(\"spark.dynamicAllocation.maxExecutors\", \"10\") \\\n",
        "            .config(\"spark.executor.memory\", \"8g\") \\\n",
        "            .config(\"spark.driver.memory\", \"4g\") \\\n",
        "            .config(\"spark.yarn.queue\", \"default\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "        # Set log level\n",
        "        self.spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "        print(f\"Spark session created successfully\")\n",
        "        print(f\"Spark version: {self.spark.version}\")\n",
        "        print(f\"Master URL: {self.spark.sparkContext.master}\")\n",
        "\n",
        "        return self.spark\n",
        "\n",
        "    def load_all_data(self):\n",
        "        \"\"\"Load ALL CSV files from GCS bucket\"\"\"\n",
        "        bucket_path = \"gs://patentbucket-maam/Data\"\n",
        "        file_pattern = \"data-*.csv\"\n",
        "\n",
        "        print(f\"Loading data from: {bucket_path}/{file_pattern}\")\n",
        "\n",
        "        # Define schema for better performance\n",
        "        schema = StructType([\n",
        "            StructField(\"patent_id\", StringType(), True),\n",
        "            StructField(\"type\", StringType(), True),\n",
        "            StructField(\"patent_number\", StringType(), True),\n",
        "            StructField(\"patent_country\", StringType(), True),\n",
        "            StructField(\"patent_date\", DateType(), True),\n",
        "            StructField(\"abstract\", StringType(), True),\n",
        "            StructField(\"title\", StringType(), True),\n",
        "            StructField(\"kind\", StringType(), True),\n",
        "            StructField(\"num_claims\", IntegerType(), True),\n",
        "            StructField(\"filename\", StringType(), True),\n",
        "            StructField(\"withdrawn\", StringType(), True),\n",
        "            StructField(\"classification_uuid\", StringType(), True),\n",
        "            StructField(\"classification_level\", StringType(), True),\n",
        "            StructField(\"section\", StringType(), True),\n",
        "            StructField(\"ipc_class\", StringType(), True),\n",
        "            StructField(\"subclass\", StringType(), True),\n",
        "            StructField(\"main_group\", StringType(), True),\n",
        "            StructField(\"subgroup\", StringType(), True),\n",
        "            StructField(\"symbol_position\", StringType(), True),\n",
        "            StructField(\"classification_value\", StringType(), True),\n",
        "            StructField(\"classification_status\", StringType(), True),\n",
        "            StructField(\"classification_data_source\", StringType(), True),\n",
        "            StructField(\"action_date\", DateType(), True),\n",
        "            StructField(\"ipc_version_indicator\", StringType(), True),\n",
        "            StructField(\"sequence\", IntegerType(), True),\n",
        "            StructField(\"assignee_id\", StringType(), True),\n",
        "            StructField(\"location_id\", StringType(), True),\n",
        "            StructField(\"city\", StringType(), True),\n",
        "            StructField(\"state\", StringType(), True),\n",
        "            StructField(\"assignee_country\", StringType(), True),\n",
        "            StructField(\"latitude\", DoubleType(), True),\n",
        "            StructField(\"longitude\", DoubleType(), True),\n",
        "            StructField(\"county\", StringType(), True),\n",
        "            StructField(\"state_fips\", StringType(), True),\n",
        "            StructField(\"county_fips\", StringType(), True)\n",
        "        ])\n",
        "\n",
        "        # Read all CSV files\n",
        "        self.df = self.spark.read \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"inferSchema\", \"false\") \\\n",
        "            .schema(schema) \\\n",
        "            .csv(f\"{bucket_path}/{file_pattern}\")\n",
        "\n",
        "        self.total_rows = self.df.count()\n",
        "\n",
        "        print(f\"✓ Successfully loaded data\")\n",
        "        print(f\"✓ Total records: {self.total_rows:,}\")\n",
        "        print(f\"✓ Total partitions: {self.df.rdd.getNumPartitions()}\")\n",
        "        print(f\"✓ Total columns: {len(self.df.columns)}\")\n",
        "\n",
        "        # Register as temp view for SQL queries\n",
        "        self.df.createOrReplaceTempView(\"patents\")\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def basic_info(self):\n",
        "        \"\"\"Display basic dataset information\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"BASIC DATASET INFORMATION\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Cache for multiple operations\n",
        "        self.df.cache()\n",
        "\n",
        "        print(f\"\\nDataset Dimensions:\")\n",
        "        print(f\"Rows: {self.total_rows:,}\")\n",
        "        print(f\"Columns: {len(self.df.columns)}\")\n",
        "\n",
        "        # Data types\n",
        "        print(f\"\\nData Types:\")\n",
        "        schema_dict = {}\n",
        "        for field in self.df.schema.fields:\n",
        "            dtype = str(field.dataType)\n",
        "            schema_dict[dtype] = schema_dict.get(dtype, 0) + 1\n",
        "\n",
        "        for dtype, count in schema_dict.items():\n",
        "            print(f\"  {dtype}: {count} columns\")\n",
        "\n",
        "        # Missing values analysis\n",
        "        print(f\"\\nMissing Values Analysis (Top 20 columns):\")\n",
        "\n",
        "        # Calculate missing percentages\n",
        "        missing_expr = [\n",
        "            (count(when(col(c).isNull(), c)) / self.total_rows * 100).alias(c)\n",
        "            for c in self.df.columns\n",
        "        ]\n",
        "\n",
        "        missing_df = self.df.select(missing_expr).collect()[0]\n",
        "        missing_dict = missing_df.asDict()\n",
        "\n",
        "        # Convert to pandas for sorting and display\n",
        "        missing_pd = pd.DataFrame.from_dict(missing_dict, orient='index', columns=['Missing_Percentage'])\n",
        "        missing_pd = missing_pd.sort_values('Missing_Percentage', ascending=False)\n",
        "\n",
        "        print(missing_pd.head(20))\n",
        "\n",
        "        # Save missing values to CSV\n",
        "        missing_pd.to_csv('missing_values_summary.csv')\n",
        "        print(f\"\\n✓ Missing values summary saved to 'missing_values_summary.csv'\")\n",
        "\n",
        "        # Plot missing values\n",
        "        self._plot_missing_values(missing_pd.head(20))\n",
        "\n",
        "    def _plot_missing_values(self, missing_pd):\n",
        "        \"\"\"Plot missing values\"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        missing_sorted = missing_pd.sort_values('Missing_Percentage', ascending=True)\n",
        "\n",
        "        bars = plt.barh(range(len(missing_sorted)), missing_sorted['Missing_Percentage'])\n",
        "        plt.yticks(range(len(missing_sorted)), missing_sorted.index)\n",
        "        plt.xlabel('Missing Percentage (%)')\n",
        "        plt.title('Top 20 Columns with Missing Values')\n",
        "        plt.xlim(0, 100)\n",
        "\n",
        "        # Add value labels\n",
        "        for i, (idx, row) in enumerate(missing_sorted.iterrows()):\n",
        "            plt.text(row['Missing_Percentage'] + 1, i,\n",
        "                    f'{row[\"Missing_Percentage\"]:.1f}%', va='center')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('missing_values_plot.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"✓ Missing values plot saved to 'missing_values_plot.png'\")\n",
        "        plt.close()\n",
        "\n",
        "    def patent_date_analysis(self):\n",
        "        \"\"\"Analyze patent date information\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"PATENT DATE ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Date range and completeness\n",
        "        date_stats = self.df.select(\n",
        "            min(\"patent_date\").alias(\"min_date\"),\n",
        "            max(\"patent_date\").alias(\"max_date\"),\n",
        "            count(\"patent_date\").alias(\"non_null_dates\"),\n",
        "            count(\"*\").alias(\"total_records\")\n",
        "        ).collect()[0]\n",
        "\n",
        "        print(f\"Patent Date Range:\")\n",
        "        print(f\"Min Date: {date_stats['min_date']}\")\n",
        "        print(f\"Max Date: {date_stats['max_date']}\")\n",
        "\n",
        "        completeness = (date_stats['non_null_dates'] / date_stats['total_records']) * 100\n",
        "        print(f\"\\nPatent Date Completeness:\")\n",
        "        print(f\"Non-NULL records: {date_stats['non_null_dates']:,}\")\n",
        "        print(f\"NULL records: {date_stats['total_records'] - date_stats['non_null_dates']:,}\")\n",
        "        print(f\"Completeness: {completeness:.2f}%\")\n",
        "\n",
        "        # Year distribution\n",
        "        yearly_stats = self.df.filter(col(\"patent_date\").isNotNull()) \\\n",
        "            .withColumn(\"year\", year(\"patent_date\")) \\\n",
        "            .groupBy(\"year\") \\\n",
        "            .agg(count(\"*\").alias(\"patent_count\")) \\\n",
        "            .orderBy(\"year\")\n",
        "\n",
        "        # Convert to pandas for plotting\n",
        "        yearly_pd = yearly_stats.toPandas()\n",
        "\n",
        "        # Plot year distribution\n",
        "        self._plot_year_distribution(yearly_pd)\n",
        "\n",
        "        # Save yearly stats\n",
        "        yearly_pd.to_csv('yearly_patent_distribution.csv', index=False)\n",
        "        print(f\"\\n✓ Yearly distribution saved to 'yearly_patent_distribution.csv'\")\n",
        "\n",
        "    def _plot_year_distribution(self, yearly_pd):\n",
        "        \"\"\"Plot year distribution of patents\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        # Year distribution bar plot\n",
        "        axes[0, 0].bar(yearly_pd['year'], yearly_pd['patent_count'], alpha=0.7)\n",
        "        axes[0, 0].set_xlabel('Year')\n",
        "        axes[0, 0].set_ylabel('Number of Patents')\n",
        "        axes[0, 0].set_title('Patent Distribution by Year')\n",
        "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Cumulative distribution\n",
        "        yearly_pd['cumulative'] = yearly_pd['patent_count'].cumsum()\n",
        "        axes[0, 1].plot(yearly_pd['year'], yearly_pd['cumulative'], 'b-', linewidth=2)\n",
        "        axes[0, 1].set_xlabel('Year')\n",
        "        axes[0, 1].set_ylabel('Cumulative Patents')\n",
        "        axes[0, 1].set_title('Cumulative Patent Count')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Decade analysis\n",
        "        yearly_pd['decade'] = (yearly_pd['year'] // 10) * 10\n",
        "        decade_counts = yearly_pd.groupby('decade')['patent_count'].sum()\n",
        "        axes[1, 0].pie(decade_counts.values, labels=decade_counts.index, autopct='%1.1f%%')\n",
        "        axes[1, 0].set_title('Patent Distribution by Decade')\n",
        "\n",
        "        # Monthly trend\n",
        "        monthly_df = self.df.filter(col(\"patent_date\").isNotNull()) \\\n",
        "            .withColumn(\"month\", month(\"patent_date\")) \\\n",
        "            .groupBy(\"month\") \\\n",
        "            .agg(count(\"*\").alias(\"patent_count\")) \\\n",
        "            .orderBy(\"month\")\n",
        "\n",
        "        monthly_pd = monthly_df.toPandas()\n",
        "        axes[1, 1].plot(monthly_pd['month'], monthly_pd['patent_count'], 'g-', linewidth=2, marker='o')\n",
        "        axes[1, 1].set_xlabel('Month')\n",
        "        axes[1, 1].set_ylabel('Average Patents')\n",
        "        axes[1, 1].set_title('Average Monthly Patent Count')\n",
        "        axes[1, 1].set_xticks(range(1, 13))\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('patent_date_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"✓ Patent date analysis plots saved to 'patent_date_analysis.png'\")\n",
        "        plt.close()\n",
        "\n",
        "        # Print top years\n",
        "        top_years = yearly_pd.nlargest(10, 'patent_count')\n",
        "        print(f\"\\nTop 10 Years by Patent Count:\")\n",
        "        for idx, row in top_years.iterrows():\n",
        "            percentage = (row['patent_count'] / self.total_rows) * 100\n",
        "            print(f\"  {row['year']}: {row['patent_count']:,} patents ({percentage:.2f}%)\")\n",
        "\n",
        "    def geographic_analysis(self):\n",
        "        \"\"\"Analyze geographic distribution\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"GEOGRAPHIC ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Country analysis\n",
        "        country_stats = self.df.groupBy(\"assignee_country\") \\\n",
        "            .agg(count(\"*\").alias(\"patent_count\")) \\\n",
        "            .orderBy(col(\"patent_count\").desc())\n",
        "\n",
        "        country_pd = country_stats.limit(20).toPandas()\n",
        "\n",
        "        # Plot top countries\n",
        "        self._plot_top_countries(country_pd)\n",
        "\n",
        "        # Save country stats\n",
        "        country_stats.coalesce(1).write \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .csv(\"country_stats\")\n",
        "\n",
        "        print(f\"\\n✓ Country statistics saved to 'country_stats/' directory\")\n",
        "\n",
        "        # City and state analysis\n",
        "        geo_columns = ['city', 'state', 'county']\n",
        "        for col_name in geo_columns:\n",
        "            if col_name in self.df.columns:\n",
        "                col_stats = self.df.groupBy(col_name) \\\n",
        "                    .agg(count(\"*\").alias(\"count\")) \\\n",
        "                    .orderBy(col(\"count\").desc()) \\\n",
        "                    .limit(10)\n",
        "\n",
        "                print(f\"\\nTop 10 {col_name.upper()} values:\")\n",
        "                for row in col_stats.collect():\n",
        "                    print(f\"  {row[col_name]}: {row['count']:,}\")\n",
        "\n",
        "    def _plot_top_countries(self, country_pd):\n",
        "        \"\"\"Plot top countries by patent count\"\"\"\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        bars = plt.barh(range(len(country_pd)), country_pd['patent_count'], alpha=0.7)\n",
        "        plt.yticks(range(len(country_pd)), country_pd['assignee_country'])\n",
        "        plt.xlabel('Number of Patents')\n",
        "        plt.title('Top 20 Countries by Patent Count')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "        # Add value labels\n",
        "        for i, (idx, row) in enumerate(country_pd.iterrows()):\n",
        "            plt.text(row['patent_count'] + max(country_pd['patent_count'])*0.01,\n",
        "                    i, f'{row[\"patent_count\"]:,}', va='center', fontsize=9)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('top_countries.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"✓ Top countries plot saved to 'top_countries.png'\")\n",
        "        plt.close()\n",
        "\n",
        "        # Print top countries\n",
        "        print(f\"\\nTop 10 Countries by Patent Count:\")\n",
        "        for idx, row in country_pd.head(10).iterrows():\n",
        "            percentage = (row['patent_count'] / self.total_rows) * 100\n",
        "            print(f\"  {row['assignee_country']}: {row['patent_count']:,} ({percentage:.2f}%)\")\n",
        "\n",
        "    def classification_analysis(self):\n",
        "        \"\"\"Analyze patent classifications\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CLASSIFICATION ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        classification_cols = ['section', 'ipc_class', 'subclass', 'main_group', 'subgroup']\n",
        "\n",
        "        for col_name in classification_cols:\n",
        "            if col_name in self.df.columns:\n",
        "                col_stats = self.df.groupBy(col_name) \\\n",
        "                    .agg(count(\"*\").alias(\"count\")) \\\n",
        "                    .orderBy(col(\"count\").desc()) \\\n",
        "                    .limit(10)\n",
        "\n",
        "                print(f\"\\nTop 10 {col_name.upper()} values:\")\n",
        "                for row in col_stats.collect():\n",
        "                    print(f\"  {row[col_name]}: {row['count']:,}\")\n",
        "\n",
        "        # IPC section analysis\n",
        "        if 'section' in self.df.columns:\n",
        "            section_stats = self.df.groupBy(\"section\") \\\n",
        "                .agg(count(\"*\").alias(\"patent_count\")) \\\n",
        "                .orderBy(col(\"patent_count\").desc())\n",
        "\n",
        "            section_pd = section_stats.toPandas()\n",
        "\n",
        "            # Plot section distribution\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            bars = plt.bar(range(len(section_pd)), section_pd['patent_count'], alpha=0.7)\n",
        "            plt.xticks(range(len(section_pd)), section_pd['section'])\n",
        "            plt.xlabel('IPC Section')\n",
        "            plt.ylabel('Number of Patents')\n",
        "            plt.title('IPC Section Distribution')\n",
        "            plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "            # Add value labels\n",
        "            for i, (idx, row) in enumerate(section_pd.iterrows()):\n",
        "                plt.text(i, row['patent_count'] + max(section_pd['patent_count'])*0.01,\n",
        "                        f'{row[\"patent_count\"]:,}', ha='center', fontsize=9)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('ipc_section_distribution.png', dpi=300, bbox_inches='tight')\n",
        "            print(f\"\\n✓ IPC section distribution saved to 'ipc_section_distribution.png'\")\n",
        "            plt.close()\n",
        "\n",
        "            # Save section stats\n",
        "            section_pd.to_csv('ipc_section_stats.csv', index=False)\n",
        "            print(f\"✓ IPC section statistics saved to 'ipc_section_stats.csv'\")\n",
        "\n",
        "    def numerical_analysis(self):\n",
        "        \"\"\"Analyze numerical columns\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"NUMERICAL ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        numerical_cols = ['num_claims', 'latitude', 'longitude', 'sequence']\n",
        "        available_cols = [col for col in numerical_cols if col in self.df.columns]\n",
        "\n",
        "        for col_name in available_cols:\n",
        "            print(f\"\\n{col_name.upper()} Statistics:\")\n",
        "\n",
        "            # Calculate statistics using Spark\n",
        "            stats = self.df.select(\n",
        "                count(col_name).alias(\"count\"),\n",
        "                mean(col_name).alias(\"mean\"),\n",
        "                stddev(col_name).alias(\"stddev\"),\n",
        "                min(col_name).alias(\"min\"),\n",
        "                max(col_name).alias(\"max\"),\n",
        "                percentile_approx(col_name, 0.5).alias(\"median\"),\n",
        "                percentile_approx(col_name, 0.25).alias(\"q1\"),\n",
        "                percentile_approx(col_name, 0.75).alias(\"q3\")\n",
        "            ).collect()[0]\n",
        "\n",
        "            print(f\"  Count: {stats['count']:,}\")\n",
        "            print(f\"  Mean: {stats['mean']:.2f}\")\n",
        "            print(f\"  Std Dev: {stats['stddev']:.2f}\")\n",
        "            print(f\"  Min: {stats['min']}\")\n",
        "            print(f\"  Max: {stats['max']}\")\n",
        "            print(f\"  Median: {stats['median']}\")\n",
        "            print(f\"  Q1: {stats['q1']}\")\n",
        "            print(f\"  Q3: {stats['q3']}\")\n",
        "\n",
        "            # Calculate null percentage\n",
        "            null_count = self.total_rows - stats['count']\n",
        "            null_percentage = (null_count / self.total_rows) * 100\n",
        "            print(f\"  Missing values: {null_count:,} ({null_percentage:.2f}%)\")\n",
        "\n",
        "    def correlation_analysis(self):\n",
        "        \"\"\"Perform correlation analysis on numerical columns\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CORRELATION ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Identify numerical columns\n",
        "        numerical_types = [IntegerType(), LongType(), FloatType(), DoubleType(), DecimalType()]\n",
        "        numerical_cols = []\n",
        "\n",
        "        for field in self.df.schema.fields:\n",
        "            if any(isinstance(field.dataType, num_type) for num_type in numerical_types):\n",
        "                numerical_cols.append(field.name)\n",
        "\n",
        "        print(f\"Found {len(numerical_cols)} numerical columns:\")\n",
        "        for i, col in enumerate(numerical_cols, 1):\n",
        "            null_count = self.df.filter(col(col).isNull()).count()\n",
        "            null_pct = (null_count / self.total_rows) * 100\n",
        "            print(f\"  {i:2d}. {col}: {null_pct:.1f}% missing\")\n",
        "\n",
        "        if len(numerical_cols) < 2:\n",
        "            print(\"Not enough numerical columns for correlation analysis.\")\n",
        "            return\n",
        "\n",
        "        # Sample data for correlation (to avoid memory issues)\n",
        "        sample_fraction = min(0.1, 100000 / self.total_rows)\n",
        "        print(f\"\\nSampling {sample_fraction:.1%} of data for correlation analysis...\")\n",
        "\n",
        "        df_sample = self.df.select(numerical_cols).sample(withReplacement=False,\n",
        "                                                         fraction=sample_fraction,\n",
        "                                                         seed=42)\n",
        "\n",
        "        # Convert to pandas for correlation calculation\n",
        "        print(\"Converting to pandas for correlation calculation...\")\n",
        "        df_pandas = df_sample.toPandas()\n",
        "\n",
        "        # Drop rows with NaN for correlation\n",
        "        df_clean = df_pandas.dropna()\n",
        "        print(f\"Rows for correlation: {len(df_clean):,}\")\n",
        "\n",
        "        if len(df_clean) < 100:\n",
        "            print(\"Not enough data after cleaning for correlation analysis.\")\n",
        "            return\n",
        "\n",
        "        # Calculate correlation matrix\n",
        "        print(\"Calculating correlation matrix...\")\n",
        "        corr_matrix = df_clean.corr()\n",
        "\n",
        "        # Display correlation matrix\n",
        "        print(f\"\\nCorrelation Matrix (showing |r| > 0.1):\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Get pairs with meaningful correlations\n",
        "        strong_correlations = []\n",
        "        for i in range(len(corr_matrix.columns)):\n",
        "            for j in range(i+1, len(corr_matrix.columns)):\n",
        "                corr_value = corr_matrix.iloc[i, j]\n",
        "                if abs(corr_value) > 0.1:\n",
        "                    col1 = corr_matrix.columns[i]\n",
        "                    col2 = corr_matrix.columns[j]\n",
        "                    strong_correlations.append((col1, col2, corr_value))\n",
        "\n",
        "        if strong_correlations:\n",
        "            # Sort by absolute correlation strength\n",
        "            strong_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
        "\n",
        "            print(f\"\\nStrong Correlations (|r| > 0.1):\")\n",
        "            print(f\"{'Column 1':<20} {'Column 2':<20} {'Correlation':<12} {'Strength'}\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "            for col1, col2, corr in strong_correlations[:20]:  # Show top 20\n",
        "                strength = self._get_correlation_strength(corr)\n",
        "                print(f\"{col1:<20} {col2:<20} {corr:+.3f}       {strength}\")\n",
        "        else:\n",
        "            print(\"No strong correlations found (|r| > 0.1)\")\n",
        "\n",
        "        # Plot correlation heatmap\n",
        "        self._plot_correlation_heatmap(corr_matrix)\n",
        "\n",
        "        # Save correlation results\n",
        "        self._save_correlation_results(corr_matrix, strong_correlations)\n",
        "\n",
        "    def _get_correlation_strength(self, r):\n",
        "        \"\"\"Get descriptive strength of correlation\"\"\"\n",
        "        abs_r = abs(r)\n",
        "        if abs_r >= 0.8:\n",
        "            return \"Very Strong\"\n",
        "        elif abs_r >= 0.6:\n",
        "            return \"Strong\"\n",
        "        elif abs_r >= 0.4:\n",
        "            return \"Moderate\"\n",
        "        elif abs_r >= 0.2:\n",
        "            return \"Weak\"\n",
        "        else:\n",
        "            return \"Very Weak\"\n",
        "\n",
        "    def _plot_correlation_heatmap(self, corr_matrix):\n",
        "        \"\"\"Plot correlation heatmap\"\"\"\n",
        "        print(f\"\\nPlotting correlation heatmap...\")\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        # Create mask for upper triangle\n",
        "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "        # Create heatmap\n",
        "        sns.heatmap(corr_matrix,\n",
        "                   mask=mask,\n",
        "                   annot=True,\n",
        "                   fmt='.2f',\n",
        "                   cmap='RdBu_r',\n",
        "                   center=0,\n",
        "                   square=True,\n",
        "                   linewidths=0.5,\n",
        "                   cbar_kws={\"shrink\": 0.8})\n",
        "\n",
        "        plt.title('Correlation Matrix Heatmap', fontsize=16, pad=20)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "        plt.yticks(fontsize=10)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"✓ Correlation heatmap saved to 'correlation_matrix.png'\")\n",
        "        plt.close()\n",
        "\n",
        "    def _save_correlation_results(self, corr_matrix, strong_correlations):\n",
        "        \"\"\"Save correlation results to file\"\"\"\n",
        "        # Save full correlation matrix\n",
        "        corr_matrix.to_csv('correlation_matrix.csv')\n",
        "\n",
        "        # Save strong correlations\n",
        "        if strong_correlations:\n",
        "            strong_corr_df = pd.DataFrame(strong_correlations,\n",
        "                                         columns=['Column1', 'Column2', 'Correlation'])\n",
        "            strong_corr_df['Strength'] = strong_corr_df['Correlation'].apply(self._get_correlation_strength)\n",
        "            strong_corr_df.to_csv('strong_correlations.csv', index=False)\n",
        "\n",
        "        print(f\"\\n✓ Correlation results saved:\")\n",
        "        print(f\"  - correlation_matrix.csv\")\n",
        "        if strong_correlations:\n",
        "            print(f\"  - strong_correlations.csv\")\n",
        "\n",
        "    def run_full_analysis(self):\n",
        "        \"\"\"\n",
        "        Run complete exploratory analysis\n",
        "        \"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"PATENT DATA ANALYSIS ON DATAPROC CLUSTER-AA01\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Create Spark session\n",
        "        self.create_spark_session()\n",
        "\n",
        "        # Load ALL data\n",
        "        self.load_all_data()\n",
        "\n",
        "        if self.df is None:\n",
        "            print(\"No data loaded!\")\n",
        "            return\n",
        "\n",
        "        # Run analyses\n",
        "        self.basic_info()\n",
        "        self.patent_date_analysis()\n",
        "        self.geographic_analysis()\n",
        "        self.classification_analysis()\n",
        "        self.numerical_analysis()\n",
        "        self.correlation_analysis()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"ANALYSIS COMPLETE\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Stop Spark session\n",
        "        self.spark.stop()\n",
        "        print(\"Spark session stopped.\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    analyzer = PatentDataAnalyzerSpark()\n",
        "    analyzer.run_full_analysis()"
      ],
      "metadata": {
        "id": "4FrM81bsBR2z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}